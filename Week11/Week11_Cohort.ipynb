{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11 Problem Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_mypy\n",
    "%nb_mypy On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeAlias\n",
    "from typing import Optional, Any    \n",
    "\n",
    "Number: TypeAlias = int | float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes as axes\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Cohort Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CS0.** Do the following tasks before you start with the first cohort session.\n",
    "\n",
    "**Task 1.** Paste the following functions from your previous work:\n",
    "- `get_features_targets()`\n",
    "- `normalize_z()`\n",
    "- `prepare_feature()`\n",
    "- `prepare_target()`\n",
    "- `split_data()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_z(array: np.ndarray, columns_means: Optional[np.ndarray]=None, \n",
    "                columns_stds: Optional[np.ndarray]=None) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    assert columns_means is None or columns_means.shape == (1, array.shape[1])\n",
    "    assert columns_stds is None or columns_stds.shape == (1, array.shape[1])\n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n",
    "    assert out.shape == array.shape\n",
    "    assert columns_means.shape == (1, array.shape[1])\n",
    "    assert columns_stds.shape == (1, array.shape[1])\n",
    "    return out, columns_means, columns_stds\n",
    "\n",
    "def get_features_targets(df: pd.DataFrame, \n",
    "                         feature_names: list[str], \n",
    "                         target_names: list[str]) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    ### BEGIN SOLUTION\n",
    "    df_feature: pd.DataFrame = df[feature_names]\n",
    "    df_target: pd.DataFrame = df[target_names]\n",
    "    ### END SOLUTION\n",
    "    pass\n",
    "    return df_feature, df_target\n",
    "\n",
    "def prepare_feature(np_feature: np.ndarray) -> np.ndarray:\n",
    "    ### BEGIN SOLUTION\n",
    "    # cols = len(df_feature.columns)\n",
    "    cols: int = np_feature.shape[1]\n",
    "    X: np.ndarray = np.concatenate((np.ones((np_feature.shape[0],1)), np_feature), axis=1)\n",
    "    return X\n",
    "    ### END SOLUTION\n",
    "    pass\n",
    "\n",
    "def split_data(df_feature: pd.DataFrame, \n",
    "               df_target: pd.DataFrame, \n",
    "               random_state: Optional[Number]=None, \n",
    "               test_size: float=0.5) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    ### BEGIN SOLUTION\n",
    "    indexes: pd.Index = df_feature.index\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    k: int = int(test_size * len(indexes))\n",
    "    test_index: pd.Index = np.random.choice(indexes, k, replace=False)\n",
    "    train_index: pd.Index = indexes.drop(test_index) \n",
    "    df_feature_train: pd.DataFrame = df_feature.loc[train_index, :]\n",
    "    df_feature_test: pd.DataFrame = df_feature.loc[test_index, :]\n",
    "    df_target_train: pd.DataFrame = df_target.loc[train_index, :]\n",
    "    df_target_test: pd.DataFrame = df_target.loc[test_index, :]\n",
    "    ### END SOLUTION\n",
    "    return df_feature_train, df_feature_test, df_target_train, df_target_test\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.** Load the breast cancer data from `breast_cancer_data.csv` into a Data Frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read breast_cancer_data.csv\n",
    "# df: pd.DataFrame = None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "df: pd.DataFrame = pd.read_csv('breast_cancer_data.csv')\n",
    "### END SOLUTION\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.** Do the following tasks.\n",
    "\n",
    "- Read the following columns\n",
    "    - feature: `radius_mean`\n",
    "    - target: `diagnosis`\n",
    "- Normalize the feature column using z normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the feature and the target\n",
    "# df_feature, df_target = None, None\n",
    "\n",
    "# normalize the feature\n",
    "# array_feature,_,_ = None, None, None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "df_feature, df_target = get_features_targets(df, [\"radius_mean\"], [\"diagnosis\"])\n",
    "array_feature,_,_ = normalize_z(df_feature.to_numpy())\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.** Write a function `replace_target()` to replace the `diagnosis` column with the following mapping:\n",
    "    - `M`: `1`, this means that malignant cell are indicated as `1` in our new column.\n",
    "    - `B`: `0`, this means that benign cell are indicated as `0` in our new column.\n",
    "    \n",
    "The function should takes in the following:\n",
    "\n",
    "- `df_target`: the target data frame\n",
    "- `target_name`: which is the column name of the target data frame\n",
    "- `map`: which is a dictionary containing the map\n",
    "    \n",
    "It should returns a new data frame with the same column name but with its values changed according to the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_target(df_target: pd.DataFrame, \n",
    "                   target_name: str, map_vals: dict[Any, Any]) -> pd.DataFrame:\n",
    "    ### BEGIN SOLUTION\n",
    "    df_out: pd.DataFrame = df_target.copy()\n",
    "\n",
    "    df_out.loc[:, target_name] = df_target[target_name].apply(lambda x: map_vals[x])\n",
    "    ### END SOLUTION\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target: pd.DataFrame = replace_target(df_target, \"diagnosis\", {'M': 1, 'B': 0})\n",
    "df_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.** Do the following tasks.\n",
    "- Change feature to Numpy array and append constant 1 column.\n",
    "- Change target to Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change feature data frame to numpy array and append column 1\n",
    "# feature: np.ndarray = None\n",
    "\n",
    "# change target data frame to numpy array\n",
    "# target: np.ndarray = None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "feature: np.ndarray = prepare_feature(array_feature)\n",
    "target: np.ndarray = df_target.to_numpy()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CS1.** *Logistic function:* Write a function to calculate the hypothesis using a logistic function. Recall that the hypothesis for a logistic regression model is written as:\n",
    "\n",
    "$$\\mathbf{p}(x) = \\frac{1}{1 + e^{-\\mathbf{X}\\mathbf{b}}}$$\n",
    "\n",
    "The shape of the input is as follows:\n",
    "- $\\mathbf{b}$: is a column vector for the parameters\n",
    "- $\\mathbf{X}$: is a matrix where the number of rows are the number of data points and the the number of columns must the same as the number of parameters in $\\mathbf{b}$.\n",
    "\n",
    "Note that you need to ensure that the output is a **column vector**. \n",
    "\n",
    "You can use the following functions:\n",
    "- `np.matmul(array1, array2)`: which is to perform matrix multiplication on the two numpy arrays.\n",
    "- `np.exp()`: which is to calculate the function $e^x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_logreg(X: np.ndarray, beta: np.ndarray) -> np.ndarray:\n",
    "    X: np.ndarray = X.astype(float)\n",
    "    beta: np.ndarray = beta.astype(float)\n",
    "    ### BEGIN SOLUTION\n",
    "    out: np.ndarray = 1.0/(1.0 + np.exp(np.matmul(X, -beta)))\n",
    "    return out\n",
    "    ### END SOLUTION\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta: np.ndarray = np.array([0])\n",
    "x: np.ndarray = np.array([0])\n",
    "ans: np.ndarray = calc_logreg(x, beta)\n",
    "assert ans == 0.5\n",
    "\n",
    "beta: np.ndarray  = np.array([2])\n",
    "x: np.ndarray  = np.array([40])\n",
    "ans: np.ndarray  = calc_logreg(x, beta)\n",
    "assert np.isclose(ans, 1.0)\n",
    "\n",
    "beta: np.ndarray  = np.array([2])\n",
    "x: np.ndarray  = np.array([-40])\n",
    "ans: np.ndarray  = calc_logreg(x, beta)\n",
    "assert np.isclose(ans, 0.0)\n",
    "\n",
    "beta: np.ndarray  = np.array([[1, 2, 3]])\n",
    "x: np.ndarray  = np.array([[3, 2, 1]])\n",
    "ans: np.ndarray  = calc_logreg(x, beta.T)\n",
    "assert np.isclose(ans.all(), 1.0)\n",
    "\n",
    "beta: np.ndarray  = np.array([[1, 2, 3]])\n",
    "x: np.ndarray  = np.array([[3, 2, 1], [3, 2, 1]])\n",
    "ans: np.ndarray  = calc_logreg(x, beta.T)\n",
    "assert ans.shape == (2, 1)\n",
    "assert np.isclose(ans.all(), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "beta: np.ndarray = np.array([[3, 2, 1]])\n",
    "x: np.ndarray  = np.array([[1, 2, 3]])\n",
    "ans: np.ndarray  = calc_logreg(x, beta.T)\n",
    "assert np.isclose(ans.all(), 1.0), \"failed h11,1\"\n",
    "\n",
    "beta: np.ndarray  = np.array([[3, 2, 1]])\n",
    "x: np.ndarray  = np.array([[1, 2, 3], [1, 2, 3]])\n",
    "ans: np.ndarray  = calc_logreg(x, beta.T)\n",
    "assert ans.shape == (2, 1), \"failed h11,2\"\n",
    "assert np.isclose(ans.all(), 1.0), \"failed h11,3\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CS2.** *Cost Function:* Write a function to calculate the cost function for logistic regression. Recall that the cost function for logistic regression is given by:\n",
    "\n",
    "$$J(\\beta) = -\\frac{1}{m}\\left[\\Sigma_{i=1}^m y^i \\log(p(x^i)) + (1 - y^i) \\log(1 - p(x^i))\\right]$$\n",
    "\n",
    "You can use the following function in your code:\n",
    "- `np.where(condition, then_expression, else_expression)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logreg(X: np.ndarray, \n",
    "                        y: np.ndarray,\n",
    "                        beta: np.ndarray) -> Number:\n",
    "    np.seterr(divide = 'ignore') \n",
    "    ### BEGIN SOLUTION\n",
    "    m: int = len(y)\n",
    "    J: Number = -(1/m)*np.sum(np.where(y==1, np.log(calc_logreg(X, beta)),np.log(1-calc_logreg(X, beta))))\n",
    "    \n",
    "    ### END SOLUTION\n",
    "    np.seterr(divide = 'warn')\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y: np.ndarray = np.array([[1]])\n",
    "X: np.ndarray  = np.array([[10, 40]])\n",
    "beta: np.ndarray  = np.array([[1, 1]]).T\n",
    "ans: Number = compute_cost_logreg(X, y, beta)\n",
    "print(ans)\n",
    "assert np.isclose(ans, 0)\n",
    "\n",
    "y: np.ndarray  = np.array([[0]])\n",
    "X: np.ndarray  = np.array([[10, 40]])\n",
    "beta: np.ndarray  = np.array([[-1, -1]]).T\n",
    "ans: Number = compute_cost_logreg(X, y, beta)\n",
    "print(ans)\n",
    "assert np.isclose(ans, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "y: np.ndarray  = np.array([[1]])\n",
    "X: np.ndarray  = np.array([[20, 30]])\n",
    "beta: np.ndarray  = np.array([[1, 1]]).T\n",
    "ans: Number = compute_cost_logreg(X, y, beta)\n",
    "assert np.isclose(ans, 0), \"failed h21,1\"\n",
    "\n",
    "y: np.ndarray  = np.array([[0]])\n",
    "X: np.ndarray  = np.array([[20, 30]])\n",
    "beta: np.ndarray  = np.array([[-1, -1]]).T\n",
    "ans: Number = compute_cost_logreg(X, y, beta)\n",
    "assert np.isclose(ans, 0), \"failed h21,2\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CS3.** *Gradient Descent:* Recall that the update functions can be written as a matrix multiplication.\n",
    "\n",
    "$$\\mathbf{b} = \\mathbf{b} - \\alpha\\frac{1}{m}\\mathbf{X}^T(\\mathbf{p} - \\mathbf{y}) $$\n",
    "\n",
    "Write a function called `gradient_descent_logreg()` that takes in four parameters:\n",
    "- `X`: is a 2-D numpy array for the features\n",
    "- `y`: is a vector array for the target\n",
    "- `beta`: is a column vector for the initial guess of the parameters\n",
    "- `alpha`: is the learning rate\n",
    "- `num_iters`: is the number of iteration to perform\n",
    "\n",
    "The function should return two arrays:\n",
    "- `beta`: is coefficient at the end of the iteration\n",
    "- `J_storage`: is the array that stores the cost value at each iteration\n",
    "\n",
    "The solution is similar to Linear Regression gradient descent function with two differences:\n",
    "- you need to use `log_regression()` to calculate the hypothesis\n",
    "- you need to use `compute_cost_logreg()` to calculate the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_logreg(X: np.ndarray, \n",
    "                            y: np.ndarray, \n",
    "                            beta: np.ndarray, \n",
    "                            alpha: float,\n",
    "                            num_iters: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    ### BEGIN SOLUTION\n",
    "    m: int = X.shape[0]\n",
    "    J_storage: np.ndarray = np.zeros((num_iters, 1))\n",
    "    for n in range(num_iters):\n",
    "        deriv: np.ndarray = np.matmul(X.T, (calc_logreg(X, beta) - y))\n",
    "        beta = beta - alpha * (1 / m) * deriv\n",
    "        J_storage[n] = compute_cost_logreg(X, y, beta)\n",
    "    ### END_SOLUTION \n",
    "    assert beta.shape == (X.shape[1], 1)\n",
    "    assert J_storage.shape == (num_iters, 1)\n",
    "    return beta.astype(float), J_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations: int = 1500\n",
    "alpha: float = 0.01\n",
    "beta: np.ndarray = np.zeros((2,1))\n",
    "beta, J_storage = gradient_descent_logreg(feature, target, beta, alpha, iterations)\n",
    "\n",
    "print(beta)\n",
    "assert beta.shape == (2, 1)\n",
    "assert np.isclose(beta[0][0], -0.56631, rtol=1e-3)\n",
    "assert np.isclose(beta[1][0], 1.93742, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "def prepare_beta() -> tuple[np.ndarray, np.ndarray]:\n",
    "    iterations1: int = 500\n",
    "    alpha1: float = 0.02\n",
    "    beta1: np.ndarray = np.zeros((2,1))\n",
    "    beta1, J1_storage = gradient_descent_logreg(feature, target, beta1, alpha1, iterations1)\n",
    "    return beta1, J1_storage\n",
    "\n",
    "\n",
    "beta1, J1_storage = prepare_beta()\n",
    "# print(beta1)\n",
    "assert beta1.shape == (2, 1), \"failed h31,1\"\n",
    "assert np.isclose(beta1[0][0], -0.505984, rtol=1e-3), \"failed h31,2\"\n",
    "assert np.isclose(beta1[1][0], 1.60153, rtol=1e-3), \"failed h31,3\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(J_storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CS4.** *Predict:* Write two functions `predict_logreg()` and `predict_norm()` that calculate the straight line equation given the features and its coefficient.\n",
    "- `predict_logreg()`: this function should standardize the feature using z normalization, change it to a Numpy array, and add a column of constant 1s. You should use `prepare_feature()` for this purpose. Lastly, this function should also call `predict_norm()` to get the predicted y values.\n",
    "- `predict_norm()`: this function should calculate the hypothesis or its probability using `calc_logreg()` and categorize it to either 0 or 1 based on its probability. If the probability is greater or equal to 0.5, it should be classified as class 1. Otherwise, it is classified as 0.\n",
    "\n",
    "You can use the following function in your code:\n",
    "- `np.where()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_norm(X: np.ndarray, beta: np.ndarray) -> np.ndarray:\n",
    "    ### BEGIN SOLUTION\n",
    "    p: np.ndarray = calc_logreg(X, beta)\n",
    "    return np.where(p >= 0.5, 1, 0)\n",
    "    ### END SOLUTION\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logreg(array_feature: np.ndarray, \n",
    "                   beta: np.ndarray, \n",
    "                   means: Optional[np.ndarray]=None, \n",
    "                   stds: Optional[np.ndarray]=None) -> np.ndarray:\n",
    "    ### BEGIN SOLUTION\n",
    "    norm_data, _, _ = normalize_z(array_feature, means, stds)\n",
    "    X: np.ndarray = prepare_feature(norm_data)\n",
    "    result = predict_norm(X, beta)\n",
    "    ### END SOLUTION\n",
    "    assert result.shape == (array_feature.shape[0], 1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature, df_target = get_features_targets(df, [\"radius_mean\"], [\"diagnosis\"])\n",
    "df_target: pd.DataFrame = replace_target(df_target, \"diagnosis\", {'M': 1, 'B': 0})\n",
    "pred: np.ndarray = predict_logreg(df_feature.to_numpy(), beta)\n",
    "print(pred.mean(), pred.std())\n",
    "assert isinstance(pred, np.ndarray)\n",
    "assert np.isclose(pred.mean(), 0.28998)\n",
    "assert np.isclose(pred.std(), 0.45375)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means: np.ndarray = np.array([0]).reshape(1,1)\n",
    "stds: np.ndarray = np.array([1]).reshape(1,1)\n",
    "beta: np.ndarray =np.array([[-0.56630289], [ 1.93763591]])\n",
    "input_1row: np.ndarray = np.array([[2.109139]])\n",
    "pred_1row: np.ndarray = predict_logreg(input_1row, beta, means, stds)\n",
    "assert pred_1row[0][0] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "beta1, buf = prepare_beta()\n",
    "pred1: np.ndarray = predict_logreg(array_feature, beta1)\n",
    "#print(pred1.mean(), pred1.std())\n",
    "assert isinstance(pred1, np.ndarray), \"failed h41,1\"\n",
    "assert np.isclose(pred1.mean(), 0.286465), \"failed h41,2\"\n",
    "assert np.isclose(pred1.std(), 0.45211), \"failed h41,3\"\n",
    "### END HIDDEN TESTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_feature, df_target)\n",
    "plt.scatter(df_feature, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CS5.** *Multiple features and splitting of data set:* \n",
    "\n",
    "Do the following task in the code below:\n",
    "- Read the following column names as the features: `\"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\", \"smoothness_mean\", \"compactness_mean\", \"concavity_mean\"`\n",
    "- Read the column `diagnosis` as the target. Change the value from `M` and `B` to `1` and `0` respectively.\n",
    "- Split the data set with 30% test size and `random_state = 100`.\n",
    "- Normalize the training feature data set using `normalize_z()` function.\n",
    "- Convert to numpy array both the target and the features using `prepare_feature()` and `prepare_target()` functions.\n",
    "- Call `gradient_descent()` function to get the parameters using the training data set.\n",
    "- Call `predict()` function on the test data set to get the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns: list[str] = [\"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\", \"smoothness_mean\", \"compactness_mean\", \"concavity_mean\"]\n",
    "\n",
    "# extract the features and the target columns\n",
    "# df_features, df_target = None, None\n",
    "\n",
    "# replace the target values using from string to integer 0 and 1\n",
    "# df_target: pd.DataFrame = None\n",
    "\n",
    "# split the data with random_state = 100 and 30% test size\n",
    "# df_features_train, df_features_test, df_target_train, df_target_test = None, None, None, None\n",
    "\n",
    "# normalize the features\n",
    "# array_features_train_z, means, stds = None, None, None\n",
    "\n",
    "# change the feature columns to numpy array and append column of 1s\n",
    "# features: np.ndarray = None\n",
    "\n",
    "# change the target column to numpy array\n",
    "# target: np.ndarray = None\n",
    "\n",
    "# iterations: int = 1500\n",
    "# alpha: float = 0.01\n",
    "\n",
    "# provide initial guess for beta\n",
    "# beta: np.ndarray = None\n",
    "\n",
    "# call the gradient descent method\n",
    "# beta, J_storage = None, None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "columns = [\"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\", \"smoothness_mean\", \"compactness_mean\", \"concavity_mean\"]\n",
    "df_features, df_target = get_features_targets(df, columns, [\"diagnosis\"])\n",
    "df_target: pd.DataFrame = replace_target(df_target, \"diagnosis\", {'M': 1, 'B': 0})\n",
    "\n",
    "df_features_train, df_features_test, df_target_train, df_target_test = split_data(df_features, df_target, random_state=100, test_size=0.3)\n",
    "array_features_train_z, means, stds = normalize_z(df_features_train.to_numpy())\n",
    "\n",
    "features: np.ndarray = prepare_feature(array_features_train_z)\n",
    "target: np.ndarray = df_target_train.to_numpy()\n",
    "\n",
    "iterations: int = 1500\n",
    "alpha: float = 0.01\n",
    "beta: np.ndarray = np.zeros((features.shape[1],1))\n",
    "\n",
    "beta, J_storage = gradient_descent_logreg(features, target, beta, alpha, iterations)\n",
    "### END SOLUTION\n",
    "print(beta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert beta.shape == (8, 1)\n",
    "ans: np.ndarray = np.array([[-0.6138507], \n",
    "                [ 0.8249164],\n",
    "                [ 0.7274362],\n",
    "                [ 0.8232587],\n",
    "                [ 0.8161879],\n",
    "                [ 0.5057594],\n",
    "                [ 0.4411595],\n",
    "                [ 0.7870175]])\n",
    "assert np.isclose(beta, ans).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "def prepare_beta() -> tuple[np.ndarray, np.ndarray]:\n",
    "    iterations1: int = 500\n",
    "    alpha1: float = 0.02\n",
    "    beta1: np.ndarray = np.zeros((features.shape[1],1))\n",
    "\n",
    "    beta1, J1_storage = gradient_descent_logreg(features, target, beta1, alpha1, iterations1)\n",
    "    return beta1, J1_storage\n",
    "\n",
    "beta1, J1_storage = prepare_beta()\n",
    "# print(beta1)\n",
    "assert beta1.shape == (8, 1), \"failed h51,1\"\n",
    "ans1: np.ndarray = np.array([[-0.5250625],\n",
    "                [ 0.72218777],\n",
    "                [ 0.59154237],\n",
    "                [ 0.7239136],\n",
    "                [ 0.71079841],\n",
    "                [ 0.4117684],\n",
    "                [ 0.43302351],\n",
    "                [ 0.69142309]])\n",
    "assert np.isclose(beta1, ans1).all(), \"failed h51,2\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(J_storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CS6.** Create a function `build_model_logreg()` that perform the following steps:\n",
    "- change all data to numpy array.\n",
    "- normalize the training feature data set using `normalize_z()` function.\n",
    "- create X matrix.\n",
    "- use `reshape(-1, 1)` on the target array to make sure it is a column vector. \n",
    "- run gradient descent by calling `gradient_descent_logreg()` function.\n",
    "\n",
    "This function should output `model` and `J_storage` where `model` is a dictionary containing `beta`, `means` and `stds`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_logreg(df_feature_train: pd.DataFrame,\n",
    "                       df_target_train: pd.DataFrame,\n",
    "                       beta: Optional[np.ndarray] = None,\n",
    "                       alpha: float = 0.01,\n",
    "                       iterations: int = 1500) -> tuple[dict[str, Any], np.ndarray]:\n",
    "    if beta is None:\n",
    "        beta = np.zeros((df_feature_train.shape[1] + 1, 1)) \n",
    "    assert beta.shape == (df_feature_train.shape[1] + 1, 1)\n",
    "    assert df_target_train.shape == (df_feature_train.shape[0], 1)\n",
    "    model: dict[str, Any] = {}\n",
    "    ### BEGIN SOLUTION\n",
    "    array_feature_train_z, means, stds = normalize_z(df_feature_train.to_numpy())\n",
    "    X: np.ndarray = prepare_feature(array_feature_train_z)\n",
    "    target: np.ndarray = df_target_train.to_numpy().reshape(-1,1)\n",
    "    beta, J_storage = gradient_descent_logreg(X, target, beta, alpha, iterations)\n",
    "    model = {\"beta\": beta, \"means\": means, \"stds\": stds}\n",
    "    ### END SOLUTION\n",
    "    assert model[\"beta\"].shape == (df_feature_train.shape[1] + 1, 1)\n",
    "    assert model[\"means\"].shape == (1, df_feature_train.shape[1])\n",
    "    assert model[\"stds\"].shape == (1, df_feature_train.shape[1])\n",
    "    assert J_storage.shape == (iterations, 1)\n",
    "    return model, J_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\", \"smoothness_mean\", \"compactness_mean\", \"concavity_mean\"]\n",
    "df_features, df_target = get_features_targets(df, columns, [\"diagnosis\"])\n",
    "df_target: pd.DataFrame = replace_target(df_target, \"diagnosis\", {'M': 1, 'B': 0})\n",
    "\n",
    "df_features_train, df_features_test, df_target_train, df_target_test = split_data(df_features, df_target, random_state=100, test_size=0.3)\n",
    "model, J_storage = build_model_logreg(df_features_train, df_target_train)\n",
    "\n",
    "assert isinstance(model, dict)\n",
    "assert \"beta\" in model\n",
    "assert \"means\" in model\n",
    "assert \"stds\" in model\n",
    "assert model[\"beta\"].shape == (8, 1)\n",
    "ans: np.ndarray = np.array([[-0.6138507], \n",
    "                [ 0.8249164],\n",
    "                [ 0.7274362],\n",
    "                [ 0.8232587],\n",
    "                [ 0.8161879],\n",
    "                [ 0.5057594],\n",
    "                [ 0.4411595],\n",
    "                [ 0.7870175]])\n",
    "assert np.isclose(model[\"beta\"], ans).all()\n",
    "assert np.isclose(model['means'][0, 0], 1.40347594e+01)\n",
    "assert np.isclose(model['stds'][-1, -1], 7.54400405e-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# call predict() on one record to get the predicted values\n",
    "# use the variable 'means' and 'stds' to normalize\n",
    "input_1row: np.ndarray = np.array([[12.22, 20.04, 79.47, 453.1, 0.10960, 0.11520, 0.08175]])\n",
    "\n",
    "# replace the None\n",
    "# pred_1row: np.ndarray = None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "pred_1row: np.ndarray = predict_logreg(input_1row, model[\"beta\"], model[\"means\"], model[\"stds\"])\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pred_1row[0][0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call predict() on df_features test dataset to get the predicted values\n",
    "# pred: np.ndarray = None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "pred: np.ndarray = predict_logreg(df_features_test.to_numpy(), beta, means, stds)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_features_test[\"radius_mean\"], df_target_test)\n",
    "plt.scatter(df_features_test[\"radius_mean\"], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_features_test[\"texture_mean\"], df_target_test)\n",
    "plt.scatter(df_features_test[\"texture_mean\"], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_features_test[\"perimeter_mean\"], df_target_test)\n",
    "plt.scatter(df_features_test[\"perimeter_mean\"], pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CS7.** *Confusion Matrix:* Write a function `confusion_matrix()` that takes in:\n",
    "- `ytrue`: which is the true target values\n",
    "- `ypred`: which is the predicted target values\n",
    "- `labels`: which is a list of the category. In the above case it will be `[1, 0]`. Put the positive case as the first element of the list. \n",
    "\n",
    "The function should return a dictionary containing the matrix with the following format.\n",
    "\n",
    "|                 | predicted positive (1) | predicted negative (0) |\n",
    "|-----------------|--------------------|--------------------|\n",
    "| actual positive (1) | correct positive  (1, 1) | false negative (1, 0)    |\n",
    "| actual negative (0) | false positive (0, 1)   | correct negative (0, 0)   |\n",
    "\n",
    "The keys to the dictionary are the indices: `(0, 0), (0, 1), (1, 0), (1, 1)`.\n",
    "\n",
    "You can use the following function in your code:\n",
    "- `itertools.product()`: this is to create a combination of all the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def confusion_matrix(ytrue: np.ndarray, \n",
    "                     ypred: np.ndarray, \n",
    "                     labels: list[int]) -> dict[tuple[int, int], int]:\n",
    "    output: dict[tuple[int, int], int] = {}\n",
    "    ### BEGIN SOLUTION\n",
    "    keys: itertools.product[tuple] = itertools.product(labels, repeat=2)\n",
    "\n",
    "    for k in keys:\n",
    "        output[k] = 0\n",
    "        \n",
    "    for idx in range(ytrue.shape[0]):\n",
    "        output[(ytrue[idx,0], ypred[idx,0])] +=1\n",
    "    ### END SOLUTION\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result: dict[tuple[int, int], int] = confusion_matrix(df_target_test.values, pred, [1,0])\n",
    "print(result)\n",
    "assert result == {(0, 0): 99, (0, 1): 2, (1, 0): 10, (1, 1): 59}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "beta1, J1_storage = prepare_beta()\n",
    "pred1: np.ndarray = predict_logreg(df_features_test.to_numpy(), beta1, means, stds)\n",
    "result1: dict[tuple[int, int], int] = confusion_matrix(df_target_test.values, pred1, [1,0])\n",
    "# print(result1)\n",
    "assert result1 == {(1, 1): 59, (1, 0): 10, (0, 1): 3, (0, 0): 98}, \"failed h71,1\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CS8.** *Metrics:* Write a function `calc_accuracy()` that takes in a Confusion Matrix array and output a dictionary with the following keys and values:\n",
    "- `accuracy`: total number of correct predictions / total number of records\n",
    "- `sensitivity`: total correct positive cases / total positive cases\n",
    "- `specificity`: total true negatives / total negative cases\n",
    "- `precision`: total  of correct positive cases / total predicted positive cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(cm: dict[tuple[int, int], int]) -> dict[str, float]:\n",
    "    ### BEGIN SOLUTION\n",
    "    negneg: int = cm[(0,0)]\n",
    "    pospos: int = cm[(1,1)]\n",
    "    negpos: int = cm[(0,1)]\n",
    "    posneg: int = cm[(1,0)]\n",
    "    accuracy: float = (negneg + pospos) / np.sum(list(cm.values()))\n",
    "    sensitivity: float = pospos / (pospos + posneg)\n",
    "    specificity: float = negneg / (negneg + negpos)\n",
    "    precision: float = pospos / (pospos + negpos)\n",
    "    ### END SOLUTION\n",
    "    result: dict[str, float] = {'accuracy': accuracy, 'sensitivity': sensitivity,\n",
    "              'specificity': specificity, 'precision': precision}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans: dict[str, float] = calc_accuracy(result)\n",
    "# print(ans)\n",
    "expected = {'accuracy': 0.9294, 'sensitivity': 0.8551, 'specificity': 0.9802, 'precision': 0.9672}\n",
    "assert np.isclose(ans['accuracy'], expected['accuracy'], rtol=1e-3)\n",
    "assert np.isclose(ans['sensitivity'], expected['sensitivity'], rtol=1e-3)\n",
    "assert np.isclose(ans['specificity'], expected['specificity'], rtol=1e-3)\n",
    "assert np.isclose(ans['precision'], expected['precision'], rtol=1e-3)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "beta1, J1_storage = prepare_beta()\n",
    "pred1: np.ndarray = predict_logreg(df_features_test.to_numpy(), beta1)\n",
    "result1: dict[tuple[int, int], int] = confusion_matrix(df_target_test.values, pred1, [1,0])\n",
    "# print(result)\n",
    "ans: dict[str, float] = calc_accuracy(result1)\n",
    "# print(ans)\n",
    "expected: dict[str, float] = {'accuracy': 0.9294117647058824, 'sensitivity': 0.8260869565217391, 'specificity': 1.0, 'precision': 1.0}\n",
    "assert np.isclose(ans['accuracy'], expected['accuracy']), \"failed h81,1\"\n",
    "assert np.isclose(ans['sensitivity'], expected['sensitivity']), \"failed h81,2\"\n",
    "assert np.isclose(ans['specificity'], expected['specificity']), \"failed h81,3\"\n",
    "assert np.isclose(ans['precision'], expected['precision']), \"failed h81,4\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CS9.** *Optional:* Redo the above tasks using Scikit Learn libraries. You will need to use the following:\n",
    "- [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "- [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "- [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix as cm_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns: list[str] = [\"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\", \"smoothness_mean\", \"compactness_mean\", \"concavity_mean\"]\n",
    "# get the features and the columns\n",
    "# df_features, df_target = None\n",
    "\n",
    "# replace target values with 0 and 1\n",
    "# df_target: pd.DataFrame = None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "# get the features and the targets\n",
    "df_features, df_target = get_features_targets(df, columns, [\"diagnosis\"])\n",
    "\n",
    "# replace target values with 0 and 1\n",
    "df_target: pd.DataFrame = replace_target(df_target, \"diagnosis\", {'M':1, 'B':0})\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data set using random_state = 100 and 30% test size\n",
    "# df_features_train, df_features_test, df_target_train, df_target_test = None, None, None, None\n",
    "\n",
    "# change feature to numpy array and append column of 1s\n",
    "# feature: np.ndarray = None\n",
    "\n",
    "# change target to numpy array\n",
    "# target: np.ndarray = None\n",
    "\n",
    "# this is to ensure it is either 0 or 1\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "df_features_train, df_features_test, df_target_train, df_target_test = train_test_split(df_features, df_target, random_state=100, test_size=0.3)\n",
    "\n",
    "target: np.ndarray = df_target_train.to_numpy()\n",
    "### END SOLUTION\n",
    "target: np.ndarray = target.astype(int) \n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LogisticRegression object instance, use newton-cg solver\n",
    "# model: LogisticRegression = None\n",
    "\n",
    "# build model\n",
    "# pass\n",
    "\n",
    "# get predicted value\n",
    "# pred: np.ndarray = None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "skmodel: LogisticRegression = LogisticRegression(solver='newton-cg')\n",
    "\n",
    "skmodel.fit(df_features_train.to_numpy(), target.flatten())\n",
    "pred: np.ndarray = skmodel.predict(df_features_test.to_numpy())\n",
    "### END SOLUTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate confusion matrix\n",
    "# cm: dict[tuple[int, int], int] = None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "cm: dict[tuple[int, int], int] = cm_sk(df_target_test.to_numpy().astype(int), pred, labels=[1, 0])\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected: np.ndarray = np.array([[58,  11], [6, 96]])\n",
    "assert np.array(cm == expected).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_features_test[\"radius_mean\"], df_target_test)\n",
    "plt.scatter(df_features_test[\"radius_mean\"], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_features_test[\"texture_mean\"], df_target_test)\n",
    "plt.scatter(df_features_test[\"texture_mean\"], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_features_test[\"perimeter_mean\"], df_target_test)\n",
    "plt.scatter(df_features_test[\"perimeter_mean\"], pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
