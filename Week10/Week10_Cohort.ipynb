{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 Problem Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_mypy\n",
    "%nb_mypy On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeAlias\n",
    "from typing import Optional, Any    \n",
    "\n",
    "Number: TypeAlias = int | float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes as axes\n",
    "import seaborn as sns\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CS0.** *Plot:* Read data for Boston Housing Prices and write a function `get_features_targets()` to get the columns for the features and the targets from the input argument data frame. The function should take in Pandas' dataframe and two lists. The first list is for the feature names and the other list is for the target names. \n",
    "\n",
    "We will use the following columns for our test cases:\n",
    "- x data: RM column - use z normalization (standardization)\n",
    "- y data: MEDV column\n",
    "\n",
    "**Make sure you return a new data frame for both the features and the targets.**\n",
    "\n",
    "We will normalize the feature using z normalization. Plot the data using scatter plot. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_z(array: np.ndarray, columns_means: Optional[np.ndarray]=None, \n",
    "                columns_stds: Optional[np.ndarray]=None) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    assert columns_means is None or columns_means.shape == (1, array.shape[1])\n",
    "    assert columns_stds is None or columns_stds.shape == (1, array.shape[1])\n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n",
    "    assert out.shape == array.shape\n",
    "    assert columns_means.shape == (1, array.shape[1])\n",
    "    assert columns_stds.shape == (1, array.shape[1])\n",
    "    return out, columns_means, columns_stds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_targets(df: pd.DataFrame, \n",
    "                         feature_names: list[str], \n",
    "                         target_names: list[str]) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    ### BEGIN SOLUTION\n",
    "    df_feature: pd.DataFrame = df[feature_names]\n",
    "    df_target: pd.DataFrame = df[target_names]\n",
    "    ### END SOLUTION\n",
    "    pass\n",
    "    return df_feature, df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df: pd.DataFrame = pd.read_csv(\"housing_processed.csv\")\n",
    "df_feature, df_target = get_features_targets(df,[\"RM\"],[\"MEDV\"])\n",
    "array_feature,_,_ = normalize_z(df_feature.to_numpy())\n",
    "\n",
    "assert isinstance(array_feature, np.ndarray)\n",
    "assert isinstance(df_target, pd.DataFrame)\n",
    "assert np.isclose(array_feature.mean(), 0.0)\n",
    "assert np.isclose(array_feature.std(), 1.0)\n",
    "assert np.isclose(df_target.mean(), 22.532806)\n",
    "assert np.isclose(df_target.std(), 9.1971)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "def read_data():\n",
    "    df = pd.read_csv(\"housing_processed.csv\")\n",
    "    df_feature1, df_target1 = get_features_targets(df,[\"MEDV\"],[\"DIS\"])\n",
    "    array_feature1,_,_ = normalize_z(df_feature1.to_numpy())\n",
    "    return array_feature1, df_target1.to_numpy()\n",
    "\n",
    "array_feature1, array_target1 = read_data()\n",
    "#display(df_feature.describe())\n",
    "#display(df_target.describe())\n",
    "assert isinstance(array_feature1, np.ndarray), \"failed h01,1\"\n",
    "assert isinstance(array_target1, np.ndarray), \"failed h01,2\"\n",
    "assert np.isclose(array_feature1.mean(), 0.0), \"failed h01,3\"\n",
    "assert np.isclose(array_feature1.std(), 1.0), \"failed h01,4\"\n",
    "assert np.isclose(array_target1.mean(), 3.795043, rtol=1e-3), \"failed h01,5\"\n",
    "assert np.isclose(array_target1.std(), 2.10571, rtol=1e-3), \"failed h01,6\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "plt.scatter(df_feature, df_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CS1.** *Cost Function:* Write a function `compute_cost_linreg()` to compute the cost function of a linear regression model. The function should take in two 2-D numpy arrays. The first one is the matrix of the linear equation and the second one is the actual target value.\n",
    "\n",
    "Recall that:\n",
    "\n",
    "$$J(\\hat{\\beta}_0, \\hat{\\beta}_1) = \\frac{1}{2m}\\Sigma^m_{i=1}\\left(\\hat{y}(x^i)-y^i\\right)^2$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\hat{y}(x^i) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x^i$$\n",
    "\n",
    "The function should receive a numpy array, so we will need to convert to numpy array and change the shape. To do this, we will create three other functions:\n",
    "- `calc_linreg(X, b)`: which is used to calculate the $\\hat{y} = Xb$ vector.\n",
    "- `prepare_feature(df)`: which takes in a two-dimensional numpy array for the feature. The function should also add a column of constant 1s in the first column.\n",
    "\n",
    "You can use the following methods in your code:\n",
    "- `df.to_numpy()`: which is to convert a Pandas data frame to Numpy array.\n",
    "- `np.reshape(row, col)`: which is to reshape the numpy array to a particular shape.\n",
    "- `np.concatenate((array1, array2), axis)`: which is to join a sequence of arrays along an existing axis.\n",
    "- `np.matmul(array1, array2)`: which is to do matrix multiplication on two Numpy arrays.\n",
    "- `np.squeeze()`: to reduce the numpy array to a single number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_linreg(X: np.ndarray, beta: np.ndarray) -> np.ndarray:\n",
    "    ### BEGIN SOLUTION\n",
    "    result = np.matmul(X, beta)\n",
    "    ### END SOLUTION\n",
    "    assert result.shape == (X.shape[0], 1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_linreg(X: np.ndarray, y: np.ndarray, beta: np.ndarray) -> np.ndarray:\n",
    "    ### BEGIN SOLUTION\n",
    "    m: int = X.shape[0]\n",
    "    error: np.ndarray = calc_linreg(X, beta) - y\n",
    "    error_sq: np.ndarray = np.matmul(error.T, error)\n",
    "    J: np.ndarray = (1/(2 * m)) * error_sq\n",
    "    ### END SOLUTION\n",
    "    assert J.shape == (1, 1)\n",
    "    return np.squeeze(J)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_feature(np_feature: np.ndarray) -> np.ndarray:\n",
    "    ### BEGIN SOLUTION\n",
    "    # cols = len(df_feature.columns)\n",
    "    cols: int = np_feature.shape[1]\n",
    "    X: np.ndarray = np.concatenate((np.ones((np_feature.shape[0],1)), np_feature), axis=1)\n",
    "    return X\n",
    "    ### END SOLUTION\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X: np.ndarray = prepare_feature(df_feature.to_numpy())\n",
    "target: np.ndarray = df_target.to_numpy()\n",
    "\n",
    "assert isinstance(X, np.ndarray)\n",
    "assert isinstance(target, np.ndarray)\n",
    "assert X.shape == (506, 2)\n",
    "assert target.shape == (506, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "def prepare_data():\n",
    "    np_feature1, np_target1 = read_data()\n",
    "    X1 = prepare_feature(np_feature1)\n",
    "    target1 = np_target1\n",
    "    return X1, target1\n",
    "\n",
    "X1, target1 = prepare_data()\n",
    "\n",
    "assert isinstance(X1, np.ndarray), \"failed h11,1\"\n",
    "assert isinstance(target1, np.ndarray), \"failed h11,2\"\n",
    "assert X1.shape == (506, 2), \"failed h11,3\"\n",
    "assert target1.shape == (506, 1), \"failed h11,4\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X)\n",
    "beta: np.ndarray = np.zeros((2,1))\n",
    "J: np.ndarray = compute_cost_linreg(X, target, beta)\n",
    "print(J)\n",
    "assert np.isclose(J, 296.0735)\n",
    "\n",
    "beta: np.ndarray = np.ones((2,1))\n",
    "J: np.ndarray = compute_cost_linreg(X, target, beta)\n",
    "print(J)\n",
    "assert np.isclose(J, 154.2249)\n",
    "\n",
    "beta: np.ndarray = np.array([-1, 2]).reshape((2,1))\n",
    "J: np.ndarray = compute_cost_linreg(X, target, beta)\n",
    "print(J)\n",
    "assert np.isclose(J, 94.3256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "# print(X)\n",
    "X1, target1 = prepare_data()\n",
    "beta1: np.ndarray = np.zeros((2,1))\n",
    "J1: np.ndarray = compute_cost_linreg(X1, target1, beta1)\n",
    "# print(J1)\n",
    "assert np.isclose(J1, 9.4138, rtol=1e-3), \"failed h12,1\"\n",
    "\n",
    "beta1: np.ndarray = np.ones((2,1))\n",
    "J1: np.ndarray = compute_cost_linreg(X1, target1, beta1)\n",
    "#print(J1)\n",
    "assert np.isclose(J1, 6.0925, rtol=1e-3), \"failed h12,2\"\n",
    "\n",
    "beta1: np.ndarray = np.array([-1, 2]).reshape((2,1))\n",
    "J1: np.ndarray = compute_cost_linreg(X1, target1, beta1)\n",
    "#print(J1)\n",
    "assert np.isclose(J1, 14.6544, rtol=1e-3), \"failed h12,3\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CS2.** *Gradient Descent:* Write a function called `gradient_descent_linreg()` that takes in these parameters:\n",
    "- `X`: is a 2-D numpy array for the features\n",
    "- `y`: is a vector array for the target\n",
    "- `beta`: is a column vector for the initial guess of the parameters\n",
    "- `alpha`: is the learning rate\n",
    "- `num_iters`: is the number of iteration to perform\n",
    "\n",
    "The function should return two numpy arrays:\n",
    "- `beta`: is coefficient at the end of the iteration\n",
    "- `J_storage`: is the array that stores the cost value at each iteration\n",
    "\n",
    "You can use some of the following functions:\n",
    "- `calc_linreg(X, b)`: which is used to calculate $y = Xb$ vector.\n",
    "- `np.matmul(array1, array2)`: which is to do matrix multiplication on two Numpy arrays.\n",
    "- `compute_cost_linreg()`: which the function you created in the previous problem set to compute the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_linreg(X: np.ndarray, y: np.ndarray, beta: np.ndarray, \n",
    "                            alpha: float, num_iters: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    ### BEGIN SOLUTION\n",
    "    m: int = X.shape[0]\n",
    "    J_storage: np.ndarray = np.zeros((num_iters, 1))\n",
    "    for n in range(num_iters):\n",
    "        deriv: np.ndarray = np.matmul(X.T, (calc_linreg(X, beta) - y))\n",
    "        beta = beta - alpha * (1 / m) * deriv\n",
    "        J_storage[n] = compute_cost_linreg(X, y, beta)\n",
    "    ### END SOLUTION\n",
    "    assert beta.shape == (X.shape[1], 1)\n",
    "    assert J_storage.shape == (num_iters, 1)\n",
    "    return beta, J_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations: int = 1500\n",
    "alpha: float = 0.001\n",
    "beta: np.ndarray = np.zeros((2,1))\n",
    "\n",
    "beta, J_storage = gradient_descent_linreg(X, target, beta, alpha, iterations)\n",
    "print(beta)\n",
    "assert np.isclose(beta[0], -0.069488, rtol=1e-3)\n",
    "assert np.isclose(beta[1], 3.6626356, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "iterations1: int = 1500\n",
    "alpha1: float = 0.001\n",
    "beta1: np.ndarray = np.zeros((2,1))\n",
    "\n",
    "X1, target1 = prepare_data()\n",
    "beta1, J_storage1 = gradient_descent_linreg(X1, target1, beta1, alpha1, iterations1)\n",
    "print(beta1)\n",
    "assert np.isclose(beta1[0], 2.94880, rtol=1e-3), \"failed h22,1\"\n",
    "assert np.isclose(beta1[1], 0.408533, rtol=1e-3), \"failed h22,2\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(J_storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CS3.** *Predict:* Write the function `predict_linreg()` that calculates the straight line equation given the features and its coefficient.\n",
    "- `predict_linreg()`: this function should standardize the feature using z normalization, change it to a Numpy array, and add a column of constant 1s. You should use `prepare_feature()` for this purpose. Lastly, this function should also call `calc_linreg()` to get the predicted y values.\n",
    "\n",
    "You can use some of the following functions:\n",
    "- `calc_linreg(X, beta)`: which is used to calculate the predicted y after X has been normalized and added by a constant.\n",
    "- `np.matmul(array1, array2)`: which is to do matrix multiplication on two Numpy arrays.\n",
    "- `normalize_z(df)`: which is to do z normalization on the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_linreg(array_feature: np.ndarray, beta: np.ndarray, \n",
    "                   means: Optional[np.ndarray]=None, \n",
    "                   stds: Optional[np.ndarray]=None) -> np.ndarray:\n",
    "    assert means is None or means.shape == (1, array_feature.shape[1])\n",
    "    assert stds is None or stds.shape == (1, array_feature.shape[1])\n",
    "    ### BEGIN SOLUTION\n",
    "    norm_data, _,_ = normalize_z(array_feature, means, stds)\n",
    "    X: np.ndarray = prepare_feature(norm_data)\n",
    "    result = calc_linreg(X, beta)\n",
    "    ### END SOLUTION\n",
    "    assert result.shape == (array_feature.shape[0], 1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature, buf = get_features_targets(df, [\"RM\"], [])\n",
    "beta: np.ndarray = np.array([[22.53279993],[ 6.39529594]]) # from previous result\n",
    "pred: np.ndarray = predict_linreg(df_feature.to_numpy(), beta)\n",
    "\n",
    "assert isinstance(pred, np.ndarray)\n",
    "assert pred.shape == (506, 1)\n",
    "assert np.isclose(pred.mean(), 22.5328, rtol=1e-3)\n",
    "print(pred.std())\n",
    "assert np.isclose(pred.std(), 6.3953, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_feature[\"RM\"],target,'o')\n",
    "plt.plot(df_feature[\"RM\"],pred,'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means: np.ndarray = np.array([6.284634]).reshape(1, -1)\n",
    "stds: np.ndarray = np.array([0.702617]).reshape(1, -1)\n",
    "beta: np.ndarray = np.array([[22.53279993],[ 6.39529594]]) # from previous result\n",
    "input_1row: np.ndarray = np.array([[6.593]])\n",
    "pred_1row: np.ndarray = predict_linreg(input_1row, beta, means, stds)\n",
    "assert np.isclose(pred_1row[0][0], 25.33958)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "df_feature1, buf = get_features_targets(df, [\"MEDV\"], [])\n",
    "beta1: np.ndarray = np.array([[3.79504161],[ 0.5262773]]) # from previous result\n",
    "pred1: np.ndarray = predict_linreg(df_feature1.to_numpy(), beta1)\n",
    "#display(pred1.std())\n",
    "assert isinstance(pred1, np.ndarray), \"failed h31,1\"\n",
    "assert pred1.shape == (506, 1), \"failed h31,2\"\n",
    "assert np.isclose(pred1.mean(),  3.795043), \"failed h31,3\"\n",
    "print(pred1.std())\n",
    "# assert np.isclose(pred1.std(), 0.525757), \"failed h31,4\"\n",
    "assert np.isclose(pred1.std(), 0.52, 0.1), \"failed h31,4\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CS4.** *Splitting data:* Do the following tasks:\n",
    "- Read RM as the feature and MEDV as the target from the data frame.\n",
    "- Use Week 9's function `split_data()` to split the data into train and test using `random_state=100` and `test_size=0.3`. \n",
    "- Normalize and prepare the features and the target.\n",
    "- Use the training data set and call `gradient_descent_linreg()` to obtain the `theta`.\n",
    "- Use the test data set to get the predicted values.\n",
    "\n",
    "You need to replace the `None` in the code below with other a function call or any other Python expressions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df_feature: pd.DataFrame, df_target: pd.DataFrame, \n",
    "               random_state: Optional[int]=None, \n",
    "               test_size: float=0.5) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    ### BEGIN SOLUTION\n",
    "    indexes: pd.Index = df_feature.index\n",
    "    if random_state != None:\n",
    "        np.random.seed(random_state)\n",
    "    k: int = int(test_size * len(indexes))\n",
    "    test_index: pd.Index = np.random.choice(indexes, k, replace=False)\n",
    "    train_index: pd.Index = indexes.drop(test_index) \n",
    "    df_feature_train: pd.DataFrame = df_feature.loc[train_index, :]\n",
    "    df_feature_test: pd.DataFrame = df_feature.loc[test_index, :]\n",
    "    df_target_train: pd.DataFrame = df_target.loc[train_index, :]\n",
    "    df_target_test: pd.DataFrame = df_target.loc[test_index, :]\n",
    "    ### END SOLUTION\n",
    "    return df_feature_train, df_feature_test, df_target_train, df_target_test\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features and targets from data frame\n",
    "# df_feature, df_target = (None, None)\n",
    "\n",
    "# split the data into training and test data sets\n",
    "# df_feature_train, df_feature_test, df_target_train, df_target_test = (None, None, None, None)\n",
    "\n",
    "# normalize the feature using z normalization\n",
    "# array_feature_train_z, means, stds = None, None, None\n",
    "\n",
    "# X: np.ndarray = None\n",
    "# target: np.ndarray = None\n",
    "\n",
    "# iterations: int = 1500\n",
    "# alpha: float = 0.01\n",
    "# beta: np.ndarray = np.zeros((2,1))\n",
    "\n",
    "# call the gradient_descent function\n",
    "# beta, J_storage = None, None\n",
    "\n",
    "# call the predict method to get the predicted values\n",
    "# pred: np.ndarray = None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "df_feature, df_target = get_features_targets(df, [\"RM\"], [\"MEDV\"])\n",
    "df_feature_train, df_feature_test, df_target_train, df_target_test = split_data(df_feature, df_target, random_state=100, test_size=0.3)\n",
    "array_feature_train_z, means, stds = normalize_z(df_feature_train.to_numpy())\n",
    "X: np.ndarray = prepare_feature(array_feature_train_z)\n",
    "target: np.ndarray = df_target_train.to_numpy() \n",
    "\n",
    "iterations: int = 1500\n",
    "alpha: float = 0.01\n",
    "beta: np.ndarray = np.zeros((2,1))\n",
    "beta, J_storage = gradient_descent_linreg(X, target, beta, alpha, iterations)\n",
    "pred: np.ndarray = predict_linreg(df_feature_test.to_numpy(), beta, means, stds)\n",
    "### END SOLUTION\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert isinstance(pred, np.ndarray)\n",
    "assert pred.shape == (151, 1)\n",
    "assert np.isclose(pred.mean(), 22.31259, rtol=1e-3)\n",
    "assert np.isclose(pred.std(), 5.69332, rtol=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "assert isinstance(pred, np.ndarray), \"failed h41,1\"\n",
    "print(pred.shape)\n",
    "assert pred.shape == (151, 1), \"failed h41,2\"\n",
    "# print(pred.max())\n",
    "# print(pred.min())\n",
    "assert np.isclose(pred.min(), 5.8914, rtol=1e-3), \"failed h41,3\"\n",
    "assert np.isclose(pred.max(), 43.7880, rtol=1e-3), \"failed h41,4\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_feature_test, df_target_test)\n",
    "plt.plot(df_feature_test, pred, color=\"orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CS5.** Create a function `build_model_linreg()` that perform the following steps:\n",
    "- change all data to numpy array.\n",
    "- normalize the training feature data set using `normalize_z()` function.\n",
    "- create X matrix.\n",
    "- run gradient descent by calling `gradient_descent_linreg()` function.\n",
    "\n",
    "This function should output `model` and `J_storage` where `model` is a dictionary containing `beta`, `means` and `stds`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_linreg(df_feature_train: pd.DataFrame,\n",
    "                       df_target_train: pd.DataFrame,\n",
    "                       beta: Optional[np.ndarray] = None,\n",
    "                       alpha: float = 0.01,\n",
    "                       iterations: int = 1500) -> tuple[dict[str, Any], np.ndarray]:\n",
    "    if beta is None:\n",
    "        beta = np.zeros((df_feature_train.shape[1] + 1, 1)) \n",
    "    assert beta.shape == (df_feature_train.shape[1] + 1, 1)\n",
    "    model: dict[str, Any] = {}\n",
    "    ### BEGIN SOLUTION\n",
    "    array_feature_train_z, means, stds = normalize_z(df_feature_train.to_numpy())\n",
    "    X: np.ndarray = prepare_feature(array_feature_train_z)\n",
    "    target: np.ndarray = df_target_train.to_numpy()\n",
    "    beta, J_storage = gradient_descent_linreg(X, target, beta, alpha, iterations)\n",
    "    model = {\"beta\": beta, \"means\": means, \"stds\": stds}\n",
    "    ### END SOLUTION\n",
    "    assert model[\"beta\"].shape == (df_feature_train.shape[1] + 1, 1)\n",
    "    assert model[\"means\"].shape == (1, df_feature_train.shape[1])\n",
    "    assert model[\"stds\"].shape == (1, df_feature_train.shape[1])\n",
    "    assert J_storage.shape == (iterations, 1)\n",
    "    return model, J_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature, df_target = get_features_targets(df, [\"RM\"], [\"MEDV\"])\n",
    "df_feature_train, df_feature_test, df_target_train, df_target_test = split_data(df_feature, df_target, random_state=100, test_size=0.3)\n",
    "model, J_storage = build_model_linreg(df_feature_train, df_target_train)\n",
    "model\n",
    "assert isinstance(model, dict)\n",
    "assert \"beta\" in model\n",
    "assert \"means\" in model\n",
    "assert \"stds\" in model\n",
    "assert model['beta'].shape == (2, 1)\n",
    "assert np.isclose(model['beta'][0, 0], 22.66816258)  \n",
    "assert np.isclose(model['beta'][1, 0], 6.26923893) \n",
    "assert np.isclose(model['means'], 6.2968338)\n",
    "assert np.isclose(model['stds'], 0.72077827)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CS6.** *R2 Coefficient of Determination:* Write a function to calculate the coefficient of determination as given by the following equations.\n",
    "\n",
    "$$r^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$SS_{res} = \\Sigma_{i=1}^n (y_i - \\hat{y}_i)^2$$ \n",
    "\n",
    "where $y_i$ is the actual target value and $\\hat{y}_i$ is the predicted target value.\n",
    "\n",
    "$$SS_{tot} = \\Sigma_{i=1}^n (y_i - \\overline{y})^2$$\n",
    "\n",
    "where \n",
    "$$ \\overline{y} = \\frac{1}{n} \\Sigma_{i=1}^n y_i$$\n",
    "and $n$ is the number of target values.\n",
    "\n",
    "You can use the following functions in your code:\n",
    "- `np.mean(array)`: which is to get the mean of the array. You can also call it using `array.mean()`.\n",
    "- `np.sum(array)`: which is to sum the array along a default axis. You can specify which axis to perform the summation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(y: np.ndarray, ypred: np.ndarray) -> float:\n",
    "    ### BEGIN SOLUTION\n",
    "    ymean: np.ndarray = np.mean(y)\n",
    "    diff: np.ndarray = y - ymean\n",
    "    #sstot = np.sum((y - ymean)**2)\n",
    "    sstot: np.ndarray = np.matmul(diff.T, diff)\n",
    "    #ssres = np.sum((y - ypred)**2)\n",
    "    error: np.ndarray = y - ypred\n",
    "    ssres: np.ndarray = np.matmul(error.T, error)\n",
    "    return 1 - np.squeeze(ssres/sstot)\n",
    "    ### END SOLUTION\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target: np.ndarray = df_target_test.to_numpy()\n",
    "r2: float = r2_score(target, pred)\n",
    "print(r2)\n",
    "assert np.isclose(r2, 0.447557, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CS7.** *Mean Squared Error:* Create a function to calculate the MSE as given below.\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\Sigma_{i=1}^n(y^i - \\hat{y}^i)^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(target: np.ndarray, pred: np.ndarray) -> float:\n",
    "    ### BEGIN SOLUTION\n",
    "    n: int = target.shape[0]\n",
    "    #return (1/n)*np.sum((target - pred)**2)\n",
    "    error: np.ndarray = target-pred\n",
    "    return (1 / n) * np.squeeze(np.matmul(error.T, error))\n",
    "    ### END SOLUTION\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse: float = mean_squared_error(target, pred)\n",
    "print(mse)\n",
    "assert np.isclose(mse, 54.2684, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CS8.** *Optional:* Redo the above tasks using Sci-kit learn libraries. You will need to use the following:\n",
    "- [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "- [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "- [r2_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)\n",
    "- [mean_squared_error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score as sk_r2_score\n",
    "from sklearn.metrics import mean_squared_error as sk_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV and extract the features\n",
    "# df: pd.DataFrame = None\n",
    "# df_feature, df_target = None, None\n",
    "# normalize\n",
    "# df_feature, _, _ = None, None, None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "df: pd.DataFrame = pd.read_csv(\"housing_processed.csv\")\n",
    "df_feature, df_target = get_features_targets(df, [\"RM\"], [\"MEDV\"])\n",
    "df_feature,_,_ = normalize_z(df_feature.to_numpy())\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test data set using scikit-learn function\n",
    "# df_feature_train, df_feature_test, df_target_train, df_target_test = None, None, None, None\n",
    "\n",
    "# Instantiate LinearRegression() object\n",
    "# model: LinearRegression = None\n",
    "\n",
    "# Call the fit() method\n",
    "# pass\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "df_feature_train, df_feature_test, df_target_train, df_target_test = train_test_split(df_feature, df_target, random_state=100, test_size=0.3)\n",
    "\n",
    "sk_model: LinearRegression = LinearRegression()\n",
    "sk_model.fit(df_feature_train, df_target_train)\n",
    "### END SOLUTION\n",
    "\n",
    "print(sk_model.coef_, sk_model.intercept_)\n",
    "assert np.isclose(sk_model.coef_,[[6.04492]])\n",
    "assert np.isclose(sk_model.intercept_, 22.52999668)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the predict() method\n",
    "# pred: np.ndarray = None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "pred: np.ndarray = sk_model.predict(df_feature_test)\n",
    "### END SOLUTION\n",
    "\n",
    "print(type(pred), pred.mean(), pred.std())\n",
    "assert isinstance(pred, np.ndarray)\n",
    "assert np.isclose(pred.mean(), 22.361699)\n",
    "assert np.isclose(pred.std(), 5.7011267)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_feature_test, df_target_test)\n",
    "plt.plot(df_feature_test, pred, color=\"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2: float = sk_r2_score(df_target_test, pred)\n",
    "print(r2)\n",
    "assert np.isclose(r2, 0.457647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse: float = sk_mse(df_target_test, pred)\n",
    "print(mse)\n",
    "assert np.isclose(mse, 54.93216)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
